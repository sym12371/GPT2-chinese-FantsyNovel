nohup: ignoring input
args:
Namespace(device='0,1', model_config='config/model_config_small.json', tokenizer_path='cache/vocab_small.txt', raw_data_path='data/train.json', tokenized_data_path='data/tokenized/', raw=True, epochs=12, batch_size=4, lr=0.00015, warmup_steps=2000, log_step=100, stride=512, gradient_accumulation=1, fp16=False, fp16_opt_level='O1', max_grad_norm=1.0, num_pieces=50, output_dir='model/', pretrained_model='model/model_epoch12', segment=False)
config:
{
  "attn_pdrop": 0.1,
  "embd_pdrop": 0.1,
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 10,
  "n_positions": 1024,
  "num_labels": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 13317
}

using device: cuda
building files
reading lines
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:11,  4.15it/s]  4%|▍         | 2/50 [00:00<00:11,  4.17it/s]  6%|▌         | 3/50 [00:00<00:11,  4.21it/s]  8%|▊         | 4/50 [00:00<00:10,  4.22it/s] 10%|█         | 5/50 [00:01<00:10,  4.24it/s] 12%|█▏        | 6/50 [00:01<00:10,  4.24it/s] 14%|█▍        | 7/50 [00:01<00:10,  4.23it/s] 16%|█▌        | 8/50 [00:01<00:09,  4.23it/s] 18%|█▊        | 9/50 [00:02<00:09,  4.24it/s] 20%|██        | 10/50 [00:02<00:09,  4.24it/s] 22%|██▏       | 11/50 [00:02<00:09,  4.23it/s] 24%|██▍       | 12/50 [00:02<00:09,  4.20it/s] 26%|██▌       | 13/50 [00:03<00:08,  4.19it/s] 28%|██▊       | 14/50 [00:03<00:08,  4.13it/s] 30%|███       | 15/50 [00:03<00:08,  4.11it/s] 32%|███▏      | 16/50 [00:03<00:08,  4.10it/s] 34%|███▍      | 17/50 [00:04<00:08,  4.07it/s] 36%|███▌      | 18/50 [00:04<00:07,  4.07it/s] 38%|███▊      | 19/50 [00:04<00:07,  4.08it/s] 40%|████      | 20/50 [00:04<00:07,  4.07it/s] 42%|████▏     | 21/50 [00:05<00:07,  4.09it/s] 44%|████▍     | 22/50 [00:05<00:06,  4.03it/s] 46%|████▌     | 23/50 [00:05<00:06,  4.03it/s] 48%|████▊     | 24/50 [00:05<00:06,  4.06it/s] 50%|█████     | 25/50 [00:06<00:06,  4.08it/s] 52%|█████▏    | 26/50 [00:06<00:05,  4.09it/s] 54%|█████▍    | 27/50 [00:06<00:05,  4.08it/s] 56%|█████▌    | 28/50 [00:06<00:05,  4.09it/s] 58%|█████▊    | 29/50 [00:07<00:05,  4.09it/s] 60%|██████    | 30/50 [00:07<00:04,  4.10it/s] 62%|██████▏   | 31/50 [00:07<00:04,  4.11it/s] 64%|██████▍   | 32/50 [00:07<00:04,  4.11it/s] 66%|██████▌   | 33/50 [00:07<00:04,  4.12it/s] 68%|██████▊   | 34/50 [00:08<00:03,  4.13it/s] 70%|███████   | 35/50 [00:08<00:03,  4.14it/s] 72%|███████▏  | 36/50 [00:08<00:03,  4.14it/s] 74%|███████▍  | 37/50 [00:08<00:03,  4.13it/s] 76%|███████▌  | 38/50 [00:09<00:02,  4.13it/s] 78%|███████▊  | 39/50 [00:09<00:02,  4.14it/s] 80%|████████  | 40/50 [00:09<00:02,  4.14it/s] 82%|████████▏ | 41/50 [00:09<00:02,  4.14it/s] 84%|████████▍ | 42/50 [00:10<00:01,  4.12it/s] 86%|████████▌ | 43/50 [00:10<00:01,  4.12it/s] 88%|████████▊ | 44/50 [00:10<00:01,  4.13it/s] 90%|█████████ | 45/50 [00:10<00:01,  4.13it/s] 92%|█████████▏| 46/50 [00:11<00:00,  4.13it/s] 94%|█████████▍| 47/50 [00:11<00:00,  4.13it/s] 96%|█████████▌| 48/50 [00:11<00:00,  4.13it/s] 98%|█████████▊| 49/50 [00:11<00:00,  4.12it/s]100%|██████████| 50/50 [00:12<00:00,  4.12it/s]100%|██████████| 50/50 [00:12<00:00,  4.13it/s]
finish
files built
calculating total steps
  0%|          | 0/50 [00:00<?, ?it/s] 24%|██▍       | 12/50 [00:00<00:00, 115.86it/s] 48%|████▊     | 24/50 [00:00<00:00, 117.74it/s] 72%|███████▏  | 36/50 [00:00<00:00, 118.45it/s] 96%|█████████▌| 48/50 [00:00<00:00, 118.90it/s]100%|██████████| 50/50 [00:00<00:00, 118.43it/s]
/home/shenym/gpt2/GPT2-Chinese/.venv/lib64/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
/home/shenym/gpt2/GPT2-Chinese/.venv/lib64/python3.13/site-packages/transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1691.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
total steps = 16931
Let's use 2 GPUs!
starting training
epoch 1
time: 2025-09-24 11:07:10.014562
saving model for epoch 1
epoch 1 finished
time: 2025-09-24 11:14:27.244446
time for one epoch: 0:07:17.229884
epoch 2
time: 2025-09-24 11:14:27.244507
saving model for epoch 2
epoch 2 finished
time: 2025-09-24 11:21:44.190033
time for one epoch: 0:07:16.945526
epoch 3
time: 2025-09-24 11:21:44.190112
saving model for epoch 3
epoch 3 finished
time: 2025-09-24 11:29:01.147075
time for one epoch: 0:07:16.956963
epoch 4
time: 2025-09-24 11:29:01.147161
saving model for epoch 4
epoch 4 finished
time: 2025-09-24 11:36:17.817252
time for one epoch: 0:07:16.670091
epoch 5
time: 2025-09-24 11:36:17.817297
saving model for epoch 5
epoch 5 finished
time: 2025-09-24 11:43:34.546180
time for one epoch: 0:07:16.728883
epoch 6
time: 2025-09-24 11:43:34.546220
saving model for epoch 6
epoch 6 finished
time: 2025-09-24 11:50:51.369237
time for one epoch: 0:07:16.823017
epoch 7
time: 2025-09-24 11:50:51.369273
saving model for epoch 7
epoch 7 finished
time: 2025-09-24 11:58:08.103883
time for one epoch: 0:07:16.734610
epoch 8
time: 2025-09-24 11:58:08.103919
saving model for epoch 8
epoch 8 finished
time: 2025-09-24 12:05:24.592089
time for one epoch: 0:07:16.488170
epoch 9
time: 2025-09-24 12:05:24.592124
saving model for epoch 9
epoch 9 finished
time: 2025-09-24 12:12:41.494955
time for one epoch: 0:07:16.902831
epoch 10
time: 2025-09-24 12:12:41.494994
saving model for epoch 10
epoch 10 finished
time: 2025-09-24 12:19:58.067036
time for one epoch: 0:07:16.572042
epoch 11
time: 2025-09-24 12:19:58.067075
saving model for epoch 11
epoch 11 finished
time: 2025-09-24 12:27:14.832927
time for one epoch: 0:07:16.765852
epoch 12
time: 2025-09-24 12:27:14.832965
saving model for epoch 12
epoch 12 finished
time: 2025-09-24 12:34:31.364659
time for one epoch: 0:07:16.531694
training finished
